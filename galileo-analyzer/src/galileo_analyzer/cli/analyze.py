import argparse
import asyncio
import json
from pathlib import Path
from typing import List
from ..core.config import AWSCredentials
from ..providers.factory import ProviderFactory
from ..analyzers.deep.deep_scanner import DeepScanner
from ..reporting.formatters import ReportGenerator
from ..core.models import JobAnalysisResult, JobConfig, IdleAnalysis, CostEstimate, TagsInfo, QuickCodeAnalysis, JobCategory, Priority
from ..utils.logger import setup_logger
from datetime import datetime


def main():
    """Entry point for CLI - synchronous wrapper"""
    asyncio.run(async_main())


async def async_main():
    """Async main function"""
    logger = setup_logger("galileo.deep_cli", "INFO")
    
    parser = argparse.ArgumentParser(description="Galileo Deep Analysis")
    parser.add_argument("--provider", required=True, choices=["aws"])
    parser.add_argument("--region", default="us-east-1")
    parser.add_argument("--profile", help="AWS profile")
    parser.add_argument("--candidates-file", required=True, 
                       help="Path to candidates JSON file from inventory analysis")
    parser.add_argument("--max-jobs", type=int, default=10,
                       help="Maximum number of jobs to analyze deeply")
    parser.add_argument("--output-dir", default="reports", 
                       help="Output directory for deep analysis reports")
    
    args = parser.parse_args()
    
    try:
        # Setup provider
        if args.profile:
            credentials = AWSCredentials.from_profile(args.profile, args.region)
        else:
            credentials = AWSCredentials.from_env()
        
        provider = ProviderFactory.create_aws_provider(credentials)
        scanner = DeepScanner(provider)
        report_generator = ReportGenerator(args.output_dir)
        
        # Load candidates from previous analysis
        logger.info(f"Loading candidates from: {args.candidates_file}")
        candidates = load_candidates_from_file(args.candidates_file)
        
        if not candidates:
            logger.error("No candidates found in file")
            return
        
        selected_jobs = candidates[:args.max_jobs]
        logger.info(f"Selected {len(selected_jobs)} jobs for deep analysis")
        
        # Run deep analysis
        results = []
        for i, job_result in enumerate(selected_jobs, 1):
            try:
                logger.info(f"[{i}/{len(selected_jobs)}] Analyzing: {job_result.job_name}")
                result = await scanner.analyze_job_deeply(job_result)
                results.append(result)
                logger.info(f"Completed: {result.job_name}")
            except Exception as e:
                logger.error(f"Failed to analyze {job_result.job_name}: {e}")
        
        # Generate reports
        logger.info("Generating deep analysis reports...")
        report_generator.generate_deep_analysis_reports(results, args.provider, provider.region)
        
        logger.info(f"Deep analysis completed for {len(results)} jobs")
        
    except Exception as e:
        logger.exception(f"Deep analysis failed: {e}")


def load_candidates_from_file(file_path: str) -> List[JobAnalysisResult]:
    """Load candidates from JSON file generated by inventory analysis"""
    logger = setup_logger("galileo.deep_cli", "INFO")
    
    try:
        file_path = Path(file_path)
        
        if not file_path.exists():
            raise FileNotFoundError(f"Candidates file not found: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"JSON loaded successfully. Top-level keys: {list(data.keys())}")
        
        candidates_data = []
        
        # Check for the new format (your file)
        if 'candidates' in data:
            logger.info("Found candidates in new format")
            candidates_data = data['candidates']
        
        # Fallback to old format
        elif 'summary' in data and 'deep_analysis_candidates' in data['summary']:
            logger.info("Found candidates in summary format")
            candidates_data = data['summary']['deep_analysis_candidates']
        
        # Another fallback: detailed_results
        elif 'detailed_results' in data:
            logger.info("Looking for candidates in detailed_results")
            detailed_results = data['detailed_results']
            candidates_data = [
                result for result in detailed_results 
                if any(result.get('candidate_for_deep_analysis', {}).values())
            ]
        
        if not candidates_data:
            logger.error("No candidates found in any supported format")
            return []
        
        logger.info(f"Found {len(candidates_data)} candidates")
        
        # Convert to JobAnalysisResult objects
        candidates = []
        for i, candidate_data in enumerate(candidates_data):
            try:
                job_result = _dict_to_job_analysis_result(candidate_data)
                candidates.append(job_result)
                logger.info(f"Successfully converted candidate {i+1}: {job_result.job_name}")
            except Exception as e:
                logger.error(f"Failed to convert candidate {candidate_data.get('job_name', 'unknown')}: {e}")
        
        # Sort by cost (highest first)
        candidates.sort(key=lambda x: x.cost_estimate.estimated_monthly_brl, reverse=True)
        
        logger.info(f"Successfully loaded {len(candidates)} candidates")
        return candidates
        
    except Exception as e:
        logger.exception(f"Failed to load candidates file: {e}")
        raise Exception(f"Failed to load candidates file: {e}")

def _dict_to_job_analysis_result(data: dict) -> JobAnalysisResult:
    """Convert dictionary to JobAnalysisResult object"""
    
    # Handle both formats: summary candidate or detailed result
    if 'reasons' in data and 'cost' in data:
        job_name = data['job_name']
        cost = data.get('cost', 0)
        category = data.get('category', 'ACTIVE')
        reasons = data.get('reasons', [])
        
        # Safe enum conversion
        try:
            job_category = JobCategory[category]
        except KeyError:
            job_category = JobCategory.ACTIVE
        
        # Determine priority based on cost and reasons
        if 'high_cost' in reasons or cost > 500:
            priority = Priority.HIGH
        elif 'inactive_expensive' in reasons:
            priority = Priority.MEDIUM
        else:
            priority = Priority.LOW
        
        return JobAnalysisResult(
            job_name=job_name,
            timestamp=datetime.now(),
            job_config=JobConfig(),
            idle_analysis=IdleAnalysis(
                category=job_category,
                days_idle=0,
                priority=priority
            ),
            cost_estimate=CostEstimate(
                hourly_cost_usd=0,
                estimated_monthly_usd=cost / 5.2 if cost > 0 else 0,
                estimated_monthly_brl=cost
            ),
            tags_info=TagsInfo(),
            code_analysis=QuickCodeAnalysis(
                has_script=True,
                script_location="unknown",
                inferred_purpose="unknown",
                naming_issues=[]
            ),
            recent_runs_count=0,
            candidate_for_deep_analysis={
                'high_cost': 'high_cost' in reasons,
                'naming_issues': 'naming_issues' in reasons,
                'inactive_expensive': 'inactive_expensive' in reasons,
                'never_run': 'never_run' in reasons,
                'dev_in_prod': 'dev_in_prod' in reasons
            }
        )
    else:
        # This is a detailed result from detailed_results
        job_config_data = data.get('job_config', {})
        idle_data = data.get('idle_analysis', {})
        cost_data = data.get('cost_estimate', {})
        tags_data = data.get('tags_info', {})
        code_data = data.get('code_analysis', {})
        
        # Safe enum conversions
        try:
            category = JobCategory[idle_data.get('category', 'ACTIVE')]
        except KeyError:
            category = JobCategory.ACTIVE
            
        try:
            priority = Priority[idle_data.get('priority', 'MEDIUM')]
        except KeyError:
            priority = Priority.MEDIUM
        
        return JobAnalysisResult(
            job_name=data['job_name'],
            timestamp=datetime.fromisoformat(data['timestamp']) if data.get('timestamp') else datetime.now(),
            job_config=JobConfig(
                glue_version=job_config_data.get('glue_version'),
                worker_type=job_config_data.get('worker_type'),
                number_of_workers=job_config_data.get('number_of_workers'),
                timeout=job_config_data.get('timeout'),
                max_retries=job_config_data.get('max_retries'),
                max_capacity=job_config_data.get('max_capacity'),
                allocated_capacity=job_config_data.get('allocated_capacity'),
                description=job_config_data.get('description'),
                job_mode=job_config_data.get('job_mode'),
                job_type=job_config_data.get('job_type'),
                execution_class=job_config_data.get('execution_class'),
                script_type=job_config_data.get('script_type'),
                script_location=job_config_data.get('script_location'),
                avg_execution_time_minutes=job_config_data.get('avg_execution_time_minutes')
            ),
            idle_analysis=IdleAnalysis(
                category=category,
                days_idle=idle_data.get('days_idle', 0),
                priority=priority,
                last_run_status=idle_data.get('last_run_status')
            ),
            cost_estimate=CostEstimate(
                hourly_cost_usd=cost_data.get('hourly_cost_usd', 0),
                estimated_monthly_usd=cost_data.get('estimated_monthly_usd', 0),
                estimated_monthly_brl=cost_data.get('estimated_monthly_brl', 0)
            ),
            tags_info=TagsInfo(
                environment=tags_data.get('environment', 'unknown'),
                team=tags_data.get('team', 'unknown'),
                business_domain=tags_data.get('business_domain', 'unknown'),
                criticality=tags_data.get('criticality', 'unknown'),
                owner=tags_data.get('owner', 'unknown')
            ),
            code_analysis=QuickCodeAnalysis(
                has_script=code_data.get('has_script', False),
                script_location=code_data.get('script_location', 'unknown'),
                inferred_purpose=code_data.get('inferred_purpose', 'unknown'),
                naming_issues=code_data.get('naming_issues', [])
            ),
            recent_runs_count=data.get('recent_runs_count', 0),
            candidate_for_deep_analysis=data.get('candidate_for_deep_analysis', {})
        )


if __name__ == "__main__":
    main()